{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B8Zjr1zh_nR"
   },
   "source": [
    "# üì• Ingestion & Exploration Notebook (`ingest_explore.ipynb`)\n",
    "\n",
    "**Project:** Market Forecast Capstone  \n",
    "**Purpose:**  \n",
    "- Initialize data infrastructure for the project  \n",
    "- Create and connect to our local SQLite database (`data/market_data.db`)  \n",
    "- Define table schemas (prices, macro, news, etc.)  \n",
    "- Prepare for safe, rate-limited ingestion from external APIs\n",
    "\n",
    "**This notebook does NOT train models.**  \n",
    "This is the data layer: pull, align, store, sanity check.\n",
    "\n",
    "**Key ideas:**  \n",
    "- All timestamps aligned to America/New_York  \n",
    "- Trading calendar = NYSE (no weekends, no holidays in final merged view)  \n",
    "- We keep raw API payloads for reproducibility  \n",
    "- We build daily tables that can be joined cleanly by `date`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP-RRrIYiT4Q"
   },
   "source": [
    "## ‚öñÔ∏è Data Access & Rate Limit Policy\n",
    "\n",
    "We will be using:\n",
    "- **Alpha Vantage** (market data, SPY + ~100 tickers, VIX)\n",
    "  - Free tier historically allows ~5 calls/minute and ~500 calls/day.\n",
    "  - We MUST respect pacing. We'll batch symbols and sleep between calls.\n",
    "\n",
    "- **FRED (macro data)**\n",
    "  - Very lenient. We can pull full history in one request per series.\n",
    "\n",
    "- **The Guardian API / Alpha Vantage news**\n",
    "  - We'll pull historical headlines in pages.\n",
    "  - We only keep headlines in the window *(prev day 16:00 ET ‚Üí current day 09:29 ET)* for each trading day.\n",
    "\n",
    "Principles for ingestion:\n",
    "1. We never spam an API in a tight loop with no delay.\n",
    "2. We log when/what we pulled (source, timestamp, ticker).\n",
    "3. We save raw responses under `raw/` so we can reproduce later.\n",
    "4. SQLite is our source of truth for cleaned, daily-aligned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km23vnA_jtMz",
    "outputId": "c1bebd13-76eb-46eb-a5d0-20d21176fc9c"
   },
   "outputs": [],
   "source": [
    "# If you're in Colab, start by configuring git identity\n",
    "!git config --global user.name \"JoeGiwa\"\n",
    "!git config --global user.email \"joebot17@gmail.com\"\n",
    "\n",
    "# Clone your repo (replace with your actual URL)\n",
    "!git clone https://github.com/JoeGiwa/market-forecast-capstone\n",
    "\n",
    "# Move into it\n",
    "%cd market-forecast-capstone\n",
    "\n",
    "# (Optional) create a new branch for ingestion work\n",
    "!git checkout -b feature/ingest_explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7_nvwJyqk8m",
    "outputId": "2310e1f6-38ed-48b6-aacd-d2e74c3cdd6f"
   },
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HHFGF0KpNBy"
   },
   "outputs": [],
   "source": [
    "!cp .env.template .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "holQDBPgij75",
    "outputId": "96b9f5f3-f6f0-40bf-b453-d78a816789e2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables & verify keys\n",
    "from src.utils.env_loader import load_env, check_env\n",
    "load_env()\n",
    "check_env()\n",
    "\n",
    "# --- Project paths ---\n",
    "ROOT_DIR = pathlib.Path.cwd()  # assuming you're running from repo root in Colab\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "RAW_DIR = ROOT_DIR / \"raw\"\n",
    "DB_PATH = DATA_DIR / \"market_data.db\"\n",
    "\n",
    "# make sure needed folders exist\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"üìÇ Project root:\", ROOT_DIR)\n",
    "print(\"üìÇ Data dir:\", DATA_DIR)\n",
    "print(\"üìÇ Raw dir:\", RAW_DIR)\n",
    "print(\"üíæ DB path will be:\", DB_PATH)\n",
    "\n",
    "# We'll open the SQLite connection later after we define schemas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyb0IbhMiyur"
   },
   "source": [
    "## üóÑ Database Plan (SQLite @ `data/market_data.db`)\n",
    "\n",
    "We will create the following core tables:\n",
    "\n",
    "### 1. `prices`\n",
    "Daily market data for SPY, VIX, and selected S&P 500 names  \n",
    "Columns (initial draft):\n",
    "- `date` (TEXT, 'YYYY-MM-DD')\n",
    "- `symbol` (TEXT, e.g. 'SPY')\n",
    "- `open`, `high`, `low`, `close`, `volume` (REAL/INTEGER)\n",
    "- `adjusted_close` (REAL)\n",
    "- `source` (TEXT, e.g. 'alpha_vantage')\n",
    "- PRIMARY KEY (`date`, `symbol`)\n",
    "\n",
    "Later we will compute features off of this (RSI, SMA20, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `macro`\n",
    "Macro + rates data from FRED, forward-filled to each trading date\n",
    "- `date` (TEXT)\n",
    "- `cpi` (REAL)\n",
    "- `unemployment_rate` (REAL)\n",
    "- `fed_funds_rate` (REAL)\n",
    "- `term_spread` (REAL)  -- (10y - 3m yield)\n",
    "- PRIMARY KEY (`date`)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `news_raw`\n",
    "Raw news/headline entries (Guardian or Alpha Vantage news API)\n",
    "- `id` (TEXT UNIQUE)          -- headline/article ID if available\n",
    "- `published_utc` (TEXT)      -- original timestamp\n",
    "- `published_et` (TEXT)       -- converted to America/New_York\n",
    "- `headline` (TEXT)\n",
    "- `section` (TEXT)\n",
    "- `source` (TEXT)             -- 'guardian' or 'alpha_vantage'\n",
    "- `raw_json` (TEXT)           -- store raw payload for reproducibility\n",
    "\n",
    "We don't model directly from this table. We aggregate daily sentiment next.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `news_daily`\n",
    "Daily pre-market sentiment features aligned to each trading day\n",
    "- `date` (TEXT)\n",
    "- `headline_count` (INTEGER)\n",
    "- `sent_mean` (REAL)\n",
    "- `sent_median` (REAL)\n",
    "- `sent_std` (REAL)\n",
    "- `pct_pos` (REAL)\n",
    "- `pct_neg` (REAL)\n",
    "- `no_news_day` (INTEGER 0/1)\n",
    "- (optional) section-level stats, e.g. `sent_business_mean`\n",
    "- PRIMARY KEY (`date`)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `politics`\n",
    "Political / structural regime flags\n",
    "- `date` (TEXT)\n",
    "- `pres_dem` (INTEGER 0/1)\n",
    "- `is_election_day` (INTEGER 0/1)\n",
    "- `is_inauguration_day` (INTEGER 0/1)\n",
    "- `is_shutdown_window` (INTEGER 0/1)\n",
    "- PRIMARY KEY (`date`)\n",
    "\n",
    "These will be optional in modeling, but we store them here.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `targets`\n",
    "Future returns we are trying to predict\n",
    "- `date` (TEXT)\n",
    "- `target_return_1d` (REAL)\n",
    "- `updown_1d` (INTEGER 0/1)\n",
    "- `target_return_30d` (REAL)\n",
    "- `updown_30d` (INTEGER 0/1)\n",
    "- `target_return_60d` (REAL)\n",
    "- `updown_60d` (INTEGER 0/1)\n",
    "- PRIMARY KEY (`date`)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. `features`\n",
    "Engineered daily features (technical indicators, breadth, volatility structure, Prophet residuals, etc.)\n",
    "- `date` (TEXT PRIMARY KEY)\n",
    "- Columns to be added later:\n",
    "  - technical indicators (SMA20, RSI14, MACD, Bollinger, etc.)\n",
    "  - breadth stats from top 50 / bottom 50 tickers\n",
    "  - VIX level and ŒîVIX\n",
    "  - rolling correlations / autocorr\n",
    "  - macro-aligned lagged fields if we decide to include them here\n",
    "  - Prophet trend / residual\n",
    "\n",
    "---\n",
    "\n",
    "### 8. `ingestion_log`\n",
    "Audit trail for reproducibility (optional but good practice)\n",
    "- `timestamp_utc` (TEXT)\n",
    "- `source` (TEXT)           -- 'alpha_vantage', 'fred', 'guardian'\n",
    "- `action` (TEXT)           -- 'fetch_prices', 'fetch_macro', etc.\n",
    "- `notes` (TEXT)\n",
    "- no primary key required; we append\n",
    "\n",
    "This lets you prove when/how data was added, which helps in grading and in interviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xefn2kQojCD4",
    "outputId": "01877eee-f892-4691-dc04-a54000c47f22"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite (will create the file if it doesn't exist)\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 1. prices table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS prices (\n",
    "    date TEXT NOT NULL,\n",
    "    symbol TEXT NOT NULL,\n",
    "    open REAL,\n",
    "    high REAL,\n",
    "    low REAL,\n",
    "    close REAL,\n",
    "    adjusted_close REAL,\n",
    "    volume REAL,\n",
    "    source TEXT,\n",
    "    PRIMARY KEY (date, symbol)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 2. macro table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS macro (\n",
    "    date TEXT PRIMARY KEY,\n",
    "    cpi REAL,\n",
    "    unemployment_rate REAL,\n",
    "    fed_funds_rate REAL,\n",
    "    term_spread REAL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 3. news_raw table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_raw (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    published_utc TEXT,\n",
    "    published_et TEXT,\n",
    "    headline TEXT,\n",
    "    section TEXT,\n",
    "    source TEXT,\n",
    "    raw_json TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 4. news_daily table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_daily (\n",
    "    date TEXT PRIMARY KEY,\n",
    "    headline_count INTEGER,\n",
    "    sent_mean REAL,\n",
    "    sent_median REAL,\n",
    "    sent_std REAL,\n",
    "    pct_pos REAL,\n",
    "    pct_neg REAL,\n",
    "    no_news_day INTEGER\n",
    "    -- optional: add section-specific columns later\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 5. politics table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS politics (\n",
    "    date TEXT PRIMARY KEY,\n",
    "    pres_dem INTEGER,\n",
    "    is_election_day INTEGER,\n",
    "    is_inauguration_day INTEGER,\n",
    "    is_shutdown_window INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 6. targets table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS targets (\n",
    "    date TEXT PRIMARY KEY,\n",
    "    target_return_1d REAL,\n",
    "    updown_1d INTEGER,\n",
    "    target_return_30d REAL,\n",
    "    updown_30d INTEGER,\n",
    "    target_return_60d REAL,\n",
    "    updown_60d INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 7. features table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS features (\n",
    "    date TEXT PRIMARY KEY\n",
    "    -- feature columns (SMA20, RSI14, MACD, etc.) will be added later via ALTER TABLE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 8. ingestion_log table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ingestion_log (\n",
    "    timestamp_utc TEXT,\n",
    "    source TEXT,\n",
    "    action TEXT,\n",
    "    notes TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ SQLite database initialized at\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49hCmWebvTCJ"
   },
   "source": [
    "## üóÉÔ∏è Database Schema and Incremental Update Strategy\n",
    "\n",
    "This notebook initializes the unified SQLite database (`data/market_data.db`) used throughout the capstone.  \n",
    "It consolidates **market**, **macroeconomic**, **news/sentiment**, and **political** data into one warehouse for simplicity, robustness, and efficient feature joins.\n",
    "\n",
    "### Schema Overview\n",
    "| Table | Purpose | Primary Keys | Example Columns |\n",
    "|--------|----------|---------------|-----------------|\n",
    "| `prices` | Historical market data for S&P 500 tickers | `(date, symbol)` | open, high, low, close, volume, sma_20, rsi_14, etc. |\n",
    "| `macro` | FRED macroeconomic indicators | `date` | cpi, unemployment, fed_funds, term_spread |\n",
    "| `news_raw` | Raw Guardian API articles | `id` | published_at, title, url, sentiment_score |\n",
    "| `news_daily` | Aggregated daily sentiment features | `date` | mean_sentiment, article_count |\n",
    "| `politics` | Flags for key political/economic events | `date` | pres_dem, is_election_day, is_shutdown_window |\n",
    "| `features` | Engineered model-ready dataset | `(date, symbol)` | merged from prices, macro, and sentiment |\n",
    "| `targets` | Prediction targets (e.g. next-day or 30-day returns) | `(date, symbol, horizon)` | return_value, class_label |\n",
    "| `ingestion_log` | Record of data pulls | `id` | source, action, timestamp, notes |\n",
    "\n",
    "### Incremental Update Strategy\n",
    "Each table uses unique primary keys to prevent duplicates and enable incremental ingestion:\n",
    "- **Prices:** upsert by `(date, symbol)`\n",
    "- **Macro:** upsert by `date`\n",
    "- **News:** upsert by `id` (article GUID)\n",
    "- **Logs:** append-only with timestamp\n",
    "\n",
    "This design allows:\n",
    "- Partial backfills (e.g., new tickers)\n",
    "- Regular updates (e.g., daily pipeline)\n",
    "- Historical reproducibility (via ingestion_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvvotkfVyV43"
   },
   "source": [
    "## üîÑ Step 3: Ingestion Layer Setup\n",
    "\n",
    "We now prepare the helper functions that every data source (Alpha Vantage, FRED, Guardian) will use.\n",
    "\n",
    "### Why this matters\n",
    "- We want **incremental updates** (don‚Äôt re-pull 25 years every time).\n",
    "- We want **clean inserts** into SQLite without creating duplicates.\n",
    "- We want **traceability** (what we pulled, when, from which source).\n",
    "\n",
    "### Core helper functions we‚Äôre about to define\n",
    "1. `get_connection()`  \n",
    "   - Opens a connection to `data/market_data.db`\n",
    "\n",
    "2. `upsert_prices(df)` (pattern for all tables later)  \n",
    "   - Inserts new rows into the `prices` table using `(date, symbol)` as the primary key.\n",
    "   - If a row already exists for that `(date, symbol)`, we skip or replace it.\n",
    "\n",
    "   We'll build this with SQL `INSERT OR REPLACE`, so you can safely rerun the same ingestion without duplicates.\n",
    "\n",
    "3. `log_ingestion(source, action, notes=\"\")`  \n",
    "   - Appends to `ingestion_log`\n",
    "   - Lets us record what symbols, date ranges, etc. we pulled\n",
    "\n",
    "### Alpha Vantage rate limit notes\n",
    "- Free tier is historically ~5 calls/minute and ~500 calls/day.\n",
    "- We will respect this by:\n",
    "  - pulling one ticker at a time,\n",
    "  - inserting it,\n",
    "  - sleeping between tickers (we'll add the sleep in a later cell),\n",
    "  - using incremental logic to only request *missing* dates.\n",
    "\n",
    "We'll start ingestion with SPY first, test the pipeline, then scale out to VIX and the 100 constituent tickers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UlBSsMoyokn"
   },
   "outputs": [],
   "source": [
    "# === DB Utilities (canonical) ===\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the same DB_PATH defined in your Paths cell:\n",
    "\n",
    "# DB_PATH = ROOT_DIR / \"data\" / \"market_data.db\"\n",
    "\n",
    "def get_connection():\n",
    "\n",
    "    return sqlite3.connect(DB_PATH)\n",
    "\n",
    "def log_ingestion(source: str, action: str, notes: str = \"\"):\n",
    "\n",
    "    ts = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    with get_connection() as conn:\n",
    "\n",
    "        conn.execute(\n",
    "\n",
    "            \"INSERT INTO ingestion_log (timestamp_utc, source, action, notes) VALUES (?, ?, ?, ?)\",\n",
    "\n",
    "            (ts, source, action, notes),\n",
    "\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ Logged ingestion: {source} | {action} | {ts} | {notes}\")\n",
    "\n",
    "def get_last_date_for_symbol(symbol: str):\n",
    "\n",
    "    with get_connection() as conn:\n",
    "\n",
    "        row = conn.execute(\n",
    "\n",
    "            \"SELECT MAX(date) FROM prices WHERE symbol = ?;\",\n",
    "\n",
    "            (symbol,)\n",
    "\n",
    "        ).fetchone()\n",
    "\n",
    "    return row[0] if row and row[0] is not None else None\n",
    "\n",
    "def upsert_prices(df: pd.DataFrame):\n",
    "\n",
    "    required = [\"date\",\"symbol\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"adjusted_close\",\"source\"]\n",
    "\n",
    "    for col in required:\n",
    "\n",
    "        if col not in df.columns:\n",
    "\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    rows = list(df[required].itertuples(index=False, name=None))\n",
    "\n",
    "    with get_connection() as conn:\n",
    "\n",
    "        conn.executemany(\"\"\"\n",
    "\n",
    "            INSERT OR REPLACE INTO prices\n",
    "\n",
    "            (date, symbol, open, high, low, close, volume, adjusted_close, source)\n",
    "\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\n",
    "        \"\"\", rows)\n",
    "\n",
    "    print(f\"‚úÖ Upserted {len(rows)} rows into prices.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "collapsed": true,
    "id": "QkSDMA-swrsw",
    "outputId": "3d13b22e-9c30-49ae-ab61-900f4fda3341"
   },
   "outputs": [],
   "source": [
    "# --- Inspect all table schemas ---\n",
    "def inspect_table_schema(connection, table_name):\n",
    "    print(f\"\\nüß© Schema for '{table_name}':\")\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    rows = cursor.fetchall()\n",
    "    for col in rows:\n",
    "        print(f\"  {col[1]:<25} | {col[2]:<10} | {'PK' if col[5] else ''}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "for t in [\"prices\", \"macro\", \"news_raw\", \"news_daily\", \"politics\", \"features\", \"targets\", \"ingestion_log\"]:\n",
    "    inspect_table_schema(conn, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7Solnu8860O",
    "outputId": "d7fb6100-40da-496e-f020-a56cc658b1d7"
   },
   "outputs": [],
   "source": [
    "def ingest_symbol_alpaca(symbol: str, start_year: int = 2016) -> int:\n",
    "\n",
    "    import os, time, requests\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    from datetime import timezone\n",
    "\n",
    "    APCA_KEY_ID = os.getenv(\"APCA_API_KEY_ID\")\n",
    "\n",
    "    APCA_API_SECRET = os.getenv(\"APCA_API_SECRET_KEY\")\n",
    "\n",
    "    if not APCA_KEY_ID or not APCA_API_SECRET:\n",
    "\n",
    "        raise RuntimeError(\"Missing Alpaca keys in .env\")\n",
    "\n",
    "    headers = {\"APCA-API-KEY-ID\": APCA_KEY_ID, \"APCA-API-SECRET-KEY\": APCA_API_SECRET}\n",
    "\n",
    "    def iso(dt):\n",
    "\n",
    "        return pd.to_datetime(dt, utc=True).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    def fetch(symbol, start_dt, end_dt, limit_per_page=10000, sleep_sec=0.15):\n",
    "\n",
    "        url = f\"https://data.alpaca.markets/v2/stocks/{symbol}/bars\"\n",
    "\n",
    "        params = {\"timeframe\":\"1Day\",\"start\":iso(start_dt),\"end\":iso(end_dt),\"limit\":limit_per_page,\"feed\":\"iex\"}\n",
    "\n",
    "        all_rows, token = [], None\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if token: params[\"page_token\"] = token\n",
    "\n",
    "            r = requests.get(url, headers=headers, params=params, timeout=60)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "\n",
    "                raise RuntimeError(f\"HTTP {r.status_code}: {r.text}\")\n",
    "\n",
    "            payload = r.json()\n",
    "\n",
    "            for b in payload.get(\"bars\", []):\n",
    "\n",
    "                all_rows.append({\n",
    "\n",
    "                    \"date\": pd.to_datetime(b[\"t\"]).date(),\n",
    "\n",
    "                    \"open\": float(b[\"o\"]), \"high\": float(b[\"h\"]),\n",
    "\n",
    "                    \"low\": float(b[\"l\"]), \"close\": float(b[\"c\"]),\n",
    "\n",
    "                    \"volume\": float(b[\"v\"]), \"adjusted_close\": None,\n",
    "\n",
    "                })\n",
    "\n",
    "            token = payload.get(\"next_page_token\")\n",
    "\n",
    "            if not token: break\n",
    "\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "        df = pd.DataFrame(all_rows)\n",
    "\n",
    "        if df.empty: return df\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "        return df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # incremental window (with 2016 clamp)\n",
    "    last = get_last_date_for_symbol(symbol)\n",
    "    clamp = pd.Timestamp(f\"{start_year}-01-01\", tz=\"UTC\")\n",
    "\n",
    "    # tz-aware start\n",
    "    start_dt = (\n",
    "        pd.to_datetime(last).tz_localize(\"UTC\") + pd.Timedelta(days=1)\n",
    "    ) if last else clamp\n",
    "\n",
    "    # tz-aware end (always set, regardless of branch)\n",
    "    end_dt = pd.Timestamp.now(tz=\"UTC\")\n",
    "\n",
    "    # ‚úÖ Guard: nothing to fetch if we're already up to date\n",
    "    if start_dt.date() > end_dt.date():\n",
    "      print(f\"‚ÑπÔ∏è {symbol}: already up to date (start={start_dt.date()} > end={end_dt.date()}).\")\n",
    "      return 0\n",
    "\n",
    "\n",
    "    raw = fetch(symbol, start_dt, end_dt)\n",
    "    if raw.empty:\n",
    "      print(f\"‚ÑπÔ∏è No new data for {symbol}.\")\n",
    "      return 0\n",
    "\n",
    "    raw[\"symbol\"] = symbol\n",
    "\n",
    "    raw[\"source\"] = \"alpaca\"\n",
    "\n",
    "    final_df = raw[[\"date\",\"symbol\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"adjusted_close\",\"source\"]].copy()\n",
    "\n",
    "    upsert_prices(final_df)\n",
    "\n",
    "    inserted = len(final_df)\n",
    "\n",
    "    log_ingestion(\"alpaca\", \"fetch_prices\", f\"symbol={symbol} range={final_df['date'].min().date()}‚Üí{final_df['date'].max().date()} rows={inserted}\")\n",
    "\n",
    "    print(f\"‚úÖ Done: {symbol} ‚Äî inserted {inserted} rows\")\n",
    "\n",
    "    return inserted\n",
    "\n",
    "# Call it on a symbol NOT yet in DB so you see output:\n",
    "\n",
    "rows = ingest_symbol_alpaca(\"AAPL\")\n",
    "\n",
    "print(\"Inserted rows:\", rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXt5qaOvBkyk",
    "outputId": "58c83f92-a35b-4743-efe5-c2ef8c58ce94"
   },
   "outputs": [],
   "source": [
    "for sym in [\"AAPL\", \"SPY\"]:\n",
    "    ingest_symbol_alpaca(sym, start_year=2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "UX5QOigZi30W",
    "outputId": "4e341518-b8e6-46e0-8c99-b1d2acd85269"
   },
   "outputs": [],
   "source": [
    "notes = f\"symbol=SPY range={final_df['date'].min().date()}‚Üí{final_df['date'].max().date()} rows={len(final_df)}\"\n",
    "log_ingestion(source=\"alpaca\", action=\"fetch_prices\", notes=notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkTnxQkljMRU",
    "outputId": "953e06cd-36aa-4afc-a655-ef4e908e7556"
   },
   "outputs": [],
   "source": [
    "# Confirm rows landed\n",
    "import pandas as pd\n",
    "conn = get_connection()\n",
    "print(pd.read_sql_query(\"SELECT COUNT(*) AS n FROM prices WHERE symbol='SPY';\", conn))\n",
    "print(pd.read_sql_query(\"SELECT * FROM ingestion_log ORDER BY rowid DESC LIMIT 3;\", conn))\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kelheLvPxXvj"
   },
   "source": [
    "## üìà Step 4 (Alt): Ingest SPY from **Alpaca** (free tier)\n",
    "\n",
    "We‚Äôre pivoting to Alpaca‚Äôs free historical API for daily bars:\n",
    "\n",
    "- Endpoint: `GET https://data.alpaca.markets/v2/stocks/{symbol}/bars`\n",
    "- Auth: request headers  \n",
    "  - `APCA-API-KEY-ID`  \n",
    "  - `APCA-API-SECRET-KEY`\n",
    "- Params:\n",
    "  - `timeframe=1Day`\n",
    "  - `start` / `end` (ISO 8601, UTC)\n",
    "  - Pagination via `page_token`\n",
    "\n",
    "We‚Äôll:\n",
    "- Use incremental logic (start from the day **after** our latest DB date if present; else from `DATA_START_YEAR`).\n",
    "- Map Alpaca fields ‚Üí our `prices` schema:\n",
    "  - `t` ‚Üí `date`\n",
    "  - `o` ‚Üí `open`\n",
    "  - `h` ‚Üí `high`\n",
    "  - `l` ‚Üí `low`\n",
    "  - `c` ‚Üí `close`\n",
    "  - `v` ‚Üí `volume`\n",
    "  - `adjusted_close` ‚Üí `None` (not provided by the free endpoint)\n",
    "  - `source` ‚Üí `\"alpaca\"`\n",
    "\n",
    "Then we‚Äôll call `upsert_prices(df)` + `log_ingestion(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLE76PpPxYJ0",
    "outputId": "898d8619-3fbc-4b3e-a52f-bce7d6232c22"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "APCA_KEY_ID = os.getenv(\"APCA_API_KEY_ID\")\n",
    "APCA_SECRET_KEY = os.getenv(\"APCA_API_SECRET_KEY\")\n",
    "if not APCA_KEY_ID or not APCA_SECRET_KEY or \"your_\" in APCA_KEY_ID or \"your_\" in APCA_SECRET_KEY:\n",
    "    raise RuntimeError(\"Alpaca API keys missing. Please add APCA_API_KEY_ID and APCA_API_SECRET_KEY to your .env.\")\n",
    "\n",
    "SYMBOL = os.getenv(\"TICKER\", \"SPY\")\n",
    "DATA_START_YEAR = int(os.getenv(\"DATA_START_YEAR\", \"2016\"))  # your .env can still say 2000; we'll clamp below\n",
    "\n",
    "headers = {\n",
    "    \"APCA-API-KEY-ID\": APCA_KEY_ID,\n",
    "    \"APCA-API-SECRET-KEY\": APCA_SECRET_KEY,\n",
    "}\n",
    "\n",
    "def iso(dt):\n",
    "    if isinstance(dt, str):\n",
    "        return dt\n",
    "    return dt.replace(tzinfo=timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def fetch_alpaca_daily(symbol: str, start_dt: datetime, end_dt: datetime,\n",
    "                       limit_per_page: int = 10000, sleep_sec: float = 0.2):\n",
    "    \"\"\"\n",
    "    Fetch daily bars from Alpaca (IEX feed on free plan) with pagination.\n",
    "    Returns DataFrame with: date, open, high, low, close, volume, adjusted_close(None)\n",
    "    \"\"\"\n",
    "    url = f\"https://data.alpaca.markets/v2/stocks/{symbol}/bars\"\n",
    "    params = {\n",
    "        \"timeframe\": \"1Day\",\n",
    "        \"start\": iso(start_dt),\n",
    "        \"end\": iso(end_dt),\n",
    "        \"limit\": limit_per_page,\n",
    "        \"feed\": \"iex\",              # <-- IMPORTANT: free plan requires IEX feed\n",
    "        # \"adjustment\": \"raw\",      # optional; you can set \"all\" if your plan supports it\n",
    "    }\n",
    "\n",
    "    all_rows = []\n",
    "    page_token = None\n",
    "\n",
    "    while True:\n",
    "        if page_token:\n",
    "            params[\"page_token\"] = page_token\n",
    "        r = requests.get(url, headers=headers, params=params, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(f\"Alpaca request failed HTTP {r.status_code}: {r.text}\")\n",
    "        payload = r.json()\n",
    "        bars = payload.get(\"bars\", [])\n",
    "        for b in bars:\n",
    "            all_rows.append({\n",
    "                \"date\": pd.to_datetime(b[\"t\"]).date(),\n",
    "                \"open\": float(b[\"o\"]),\n",
    "                \"high\": float(b[\"h\"]),\n",
    "                \"low\": float(b[\"l\"]),\n",
    "                \"close\": float(b[\"c\"]),\n",
    "                \"volume\": float(b[\"v\"]),\n",
    "                \"adjusted_close\": None,  # not provided on free IEX endpoint\n",
    "            })\n",
    "        page_token = payload.get(\"next_page_token\")\n",
    "        if not page_token:\n",
    "            break\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---------- incremental window with clamp to IEX coverage ----------\n",
    "last_date = get_last_date_for_symbol(SYMBOL)\n",
    "iex_min_start = pd.Timestamp(\"2016-01-01\")  # free plan historical coverage (approx)\n",
    "\n",
    "if last_date:\n",
    "    start_dt = pd.to_datetime(last_date) + pd.Timedelta(days=1)\n",
    "else:\n",
    "    requested_start = pd.Timestamp(f\"{DATA_START_YEAR}-01-01\")\n",
    "    start_dt = max(requested_start, iex_min_start)  # clamp to IEX coverage\n",
    "\n",
    "end_dt = pd.Timestamp(datetime.now(timezone.utc))\n",
    "\n",
    "print(f\"üì° Alpaca (IEX): fetching {SYMBOL} 1D bars from {start_dt.date()} to {end_dt.date()} ...\")\n",
    "raw_df = fetch_alpaca_daily(SYMBOL, start_dt, end_dt)\n",
    "print(f\"‚¨áÔ∏è Pulled {len(raw_df)} rows for {SYMBOL} \"\n",
    "      f\"({raw_df['date'].min().date() if not raw_df.empty else 'NA'} ‚Üí {raw_df['date'].max().date() if not raw_df.empty else 'NA'})\")\n",
    "\n",
    "if raw_df.empty:\n",
    "    print(\"‚ÑπÔ∏è No new data from Alpaca.\")\n",
    "else:\n",
    "    raw_df[\"symbol\"] = SYMBOL\n",
    "    raw_df[\"source\"] = \"alpaca\"\n",
    "    final_df = raw_df[[\n",
    "        \"date\", \"symbol\", \"open\", \"high\", \"low\", \"close\",\n",
    "        \"volume\", \"adjusted_close\", \"source\"\n",
    "    ]].copy()\n",
    "\n",
    "    upsert_prices(final_df)\n",
    "    notes = (f\"symbol={SYMBOL} range={final_df['date'].min().date()}‚Üí{final_df['date'].max().date()} rows={len(final_df)}\")\n",
    "    log_ingestion(source=\"alpaca\", action=\"fetch_prices\", notes=notes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZFtpnGi1YfX"
   },
   "source": [
    "## üß≠ Universe: load S&P 500 list, normalize, persist\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Read the uploaded Excel once from `/mnt/data/StocksInSP500.xlsx`\n",
    "\n",
    "- Normalize column names ‚Üí (symbol, name, sector, weight)\n",
    "\n",
    "- Upsert into SQLite `universe` table (idempotent)\n",
    "\n",
    "- Save a normalized CSV to the repo: `data/reference/sp500_universe.csv` for reproducible future runs\n",
    "\n",
    "- Build Top-50 and Bottom-50 lists (by weight if available, else alphabetical)\n",
    "\n",
    "- Batch-ingest prices for the selected tickers (Alpaca, IEX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3p9hPrg1k7u",
    "outputId": "1b4af818-9b0b-4dee-db8d-fdfe716d6c2b"
   },
   "outputs": [],
   "source": [
    "with get_connection() as conn:\n",
    "\n",
    "    conn.execute(\"\"\"\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS universe (\n",
    "\n",
    "            symbol TEXT PRIMARY KEY,\n",
    "\n",
    "            name   TEXT,\n",
    "\n",
    "            sector TEXT,\n",
    "\n",
    "            weight REAL,\n",
    "\n",
    "            source TEXT\n",
    "\n",
    "        );\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ universe table ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "Jxe8XvBb1zfW",
    "outputId": "e3c38e02-18d8-4d18-8150-5128f7763a93"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "\n",
    "repo_csv = Path(\"data/reference/sp500_universe.csv\")\n",
    "\n",
    "repo_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Try repo CSV first (fast + reproducible), else read the uploaded Excel\n",
    "\n",
    "if repo_csv.exists():\n",
    "\n",
    "    dfu_raw = pd.read_csv(repo_csv)\n",
    "\n",
    "    print(f\"üìÑ Loaded repo CSV: {repo_csv}\")\n",
    "\n",
    "else:\n",
    "\n",
    "    excel_path = Path(\"/content/StocksInSP500.xlsx\")  # uploaded file path\n",
    "\n",
    "    if not excel_path.exists():\n",
    "\n",
    "        raise FileNotFoundError(f\"Excel not found at {excel_path}. Upload it or commit the repo CSV first.\")\n",
    "\n",
    "    dfu_raw = pd.read_excel(excel_path)\n",
    "\n",
    "    print(f\"üìÑ Loaded Excel: {excel_path}\")\n",
    "\n",
    "# 2) Heuristic column detection (case-insensitive)\n",
    "\n",
    "cols = {c.lower(): c for c in dfu_raw.columns}\n",
    "\n",
    "def pick(*cands):\n",
    "\n",
    "    for k in cands:\n",
    "\n",
    "        if k in cols: return cols[k]\n",
    "\n",
    "    return None\n",
    "\n",
    "col_symbol = pick(\"symbol\", \"ticker\", \"ticker symbol\", \"sp500 ticker\", \"spx ticker\")\n",
    "\n",
    "col_name   = pick(\"security\", \"name\", \"company\", \"company name\")\n",
    "\n",
    "col_sector = pick(\"gics sector\", \"sector\")\n",
    "\n",
    "col_weight = pick(\"weight\", \"index weight\", \"spx weight\", \"wgt\")\n",
    "\n",
    "if not col_symbol:\n",
    "\n",
    "    raise ValueError(f\"Ticker column not found. Available columns: {list(dfu_raw.columns)}\")\n",
    "\n",
    "dfu = dfu_raw.copy()\n",
    "\n",
    "# 3) Normalize columns ‚Üí symbol, name, sector, weight, source\n",
    "\n",
    "dfu[\"symbol\"] = dfu[col_symbol].astype(str).str.strip().str.upper()\n",
    "\n",
    "dfu[\"name\"]   = dfu[col_name] if col_name else None\n",
    "\n",
    "dfu[\"sector\"] = dfu[col_sector] if col_sector else None\n",
    "\n",
    "dfu[\"weight\"] = pd.to_numeric(dfu[col_weight], errors=\"coerce\") if col_weight else None\n",
    "\n",
    "dfu[\"source\"] = \"snp_excel\" if not repo_csv.exists() else \"repo_csv\"\n",
    "\n",
    "dfu = dfu[[\"symbol\",\"name\",\"sector\",\"weight\",\"source\"]].drop_duplicates(subset=[\"symbol\"]).reset_index(drop=True)\n",
    "\n",
    "# 4) Save normalized CSV to repo (so future loads don‚Äôt depend on Colab upload)\n",
    "\n",
    "if not repo_csv.exists():\n",
    "\n",
    "    dfu.to_csv(repo_csv, index=False)\n",
    "\n",
    "    print(f\"üíæ Saved normalized universe to {repo_csv} (commit this to git).\")\n",
    "\n",
    "print(f\"‚úÖ Universe normalized. Rows: {len(dfu)} | Columns: {list(dfu.columns)}\")\n",
    "\n",
    "dfu.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9K6U0Jl2Hi5",
    "outputId": "455d87aa-4285-40e5-a77a-007eec4cd93b"
   },
   "outputs": [],
   "source": [
    "with get_connection() as conn:\n",
    "    conn.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO universe (symbol, name, sector, weight, source)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", list(dfu.itertuples(index=False, name=None)))\n",
    "print(f\"‚úÖ Upserted {len(dfu)} rows into universe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-mPSnLZ2HL7",
    "outputId": "4b61f87d-c1b3-471d-fdb8-d2b0ea75637c"
   },
   "outputs": [],
   "source": [
    "with get_connection() as conn:\n",
    "\n",
    "    u = pd.read_sql_query(\"SELECT * FROM universe\", conn)\n",
    "\n",
    "if u[\"weight\"].notna().any():\n",
    "\n",
    "    u_sorted = u.sort_values(\"weight\", ascending=False)\n",
    "\n",
    "    top50    = u_sorted.head(50)[\"symbol\"].tolist()\n",
    "\n",
    "    bottom50 = u_sorted.tail(50)[\"symbol\"].tolist()\n",
    "\n",
    "    basis = \"weight\"\n",
    "\n",
    "else:\n",
    "\n",
    "    # fallback: alphabetical (you can replace with market cap if present later)\n",
    "\n",
    "    u_sorted = u.sort_values(\"symbol\")\n",
    "\n",
    "    top50    = u_sorted.head(50)[\"symbol\"].tolist()\n",
    "\n",
    "    bottom50 = u_sorted.tail(50)[\"symbol\"].tolist()\n",
    "\n",
    "    basis = \"alphabetical\"\n",
    "\n",
    "print(f\"‚úÖ Built Top/Bottom 50 based on: {basis}\")\n",
    "\n",
    "print(f\"Top50 sample: {top50[:10]}\")\n",
    "\n",
    "print(f\"Bottom50 sample: {bottom50[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6wh1zxK2x4x",
    "outputId": "0cbc318d-8a11-40c2-b0e7-efe270917b1b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "symbols = sorted(set(top50 + bottom50 + [\"SPY\"]))  # ensure SPY included once\n",
    "print(f\"üì° Ingesting {len(symbols)} symbols via Alpaca (IEX)...\")\n",
    "\n",
    "ok, fail = 0, []\n",
    "for i, sym in enumerate(symbols, 1):\n",
    "    try:\n",
    "        inserted = ingest_symbol_alpaca(sym, start_year=2016)\n",
    "        ok += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {sym} failed: {e}\")\n",
    "        fail.append(sym)\n",
    "    time.sleep(0.15)  # polite pacing (IEX is generous, this is just to be safe)\n",
    "\n",
    "print(f\"‚úÖ Batch complete. OK={ok}, failed={len(fail)}\")\n",
    "if fail:\n",
    "    print(\"Failed symbols:\", fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkgDIrth2xtc",
    "outputId": "8740cb96-5d73-48bb-b6f5-8a416508b569"
   },
   "outputs": [],
   "source": [
    "# Try VIX directly; if not supported on IEX free, use VIXY ETF as a proxy\n",
    "\n",
    "try:\n",
    "\n",
    "    ingest_symbol_alpaca(\"VIX\", start_year=2016)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"VIX failed on Alpaca; trying VIXY proxy...\", e)\n",
    "\n",
    "    try:\n",
    "\n",
    "        ingest_symbol_alpaca(\"VIXY\", start_year=2016)\n",
    "\n",
    "    except Exception as e2:\n",
    "\n",
    "        print(\"VIXY failed as well:\", e2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7fDYrUf000U"
   },
   "source": [
    "<details>\n",
    "<summary><b>(Legacy) Alpha Vantage ‚Äî Reference Only (Do Not Run)</b></summary>\n",
    "\n",
    "*(This section remains for archival/reference. Alpaca is our primary provider for v1. Alpha Vantage‚Äôs adjusted endpoint is now premium-only.)*\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "## üìà Step 4: Alpha Vantage Ingestion (SPY first pass)\n",
    "\n",
    "Goal of this step:\n",
    "- Pull full daily historical OHLCV for SPY from Alpha Vantage\n",
    "- Clean/standardize the data\n",
    "- Upsert it into the `prices` table in `market_data.db`\n",
    "- Log the ingestion event in `ingestion_log`\n",
    "\n",
    "Key rules:\n",
    "- We respect incremental updates.\n",
    "  - If `prices` already has SPY through some date, we'll only request data we don't have.\n",
    "- We tag all inserted rows with `source='alpha_vantage'`.\n",
    "- We will store `adjusted_close` from Alpha Vantage (important for modeling returns).\n",
    "\n",
    "Note:\n",
    "- Alpha Vantage free tier is ~5 calls/minute, ~500/day.\n",
    "- Here we're only pulling 1 ticker (SPY), so this is safe.\n",
    "\n",
    "After SPY works:\n",
    "- We'll apply the same flow to VIX and then to each ticker in the top 50 / bottom 50 list from the S&P 500 universe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "collapsed": true,
    "id": "-mZAmxrZ2eIC",
    "outputId": "a21e5d3f-0026-4993-a1f9-e60248303a36"
   },
   "outputs": [],
   "source": [
    "RUN_ALPHA = False\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------\n",
    "# Config\n",
    "# ------------\n",
    "ALPHA_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "if not ALPHA_KEY or \"your_\" in ALPHA_KEY:\n",
    "    raise RuntimeError(\"Alpha Vantage API key missing or placeholder. Please update .env and rerun load_env().\")\n",
    "\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "SYMBOL = os.getenv(\"TICKER\", \"SPY\")  # default SPY\n",
    "DATA_START_YEAR = int(os.getenv(\"DATA_START_YEAR\", \"2000\"))\n",
    "\n",
    "print(f\"üì° Fetching data for {SYMBOL} starting from {DATA_START_YEAR}...\")\n",
    "\n",
    "# ------------\n",
    "# Helper: fetch raw daily data from Alpha Vantage\n",
    "# ------------\n",
    "def fetch_alpha_vantage_daily_adjusted(symbol: str, api_key: str):\n",
    "    \"\"\"\n",
    "    Calls Alpha Vantage TIME_SERIES_DAILY_ADJUSTED for `symbol`\n",
    "    Returns a pandas DataFrame with columns:\n",
    "      date, open, high, low, close, adjusted_close, volume\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": symbol,\n",
    "        \"outputsize\": \"full\",         # full history (they usually go back ~20+ years)\n",
    "        \"datatype\": \"json\",\n",
    "        \"apikey\": api_key,\n",
    "    }\n",
    "    r = requests.get(BASE_URL, params=params)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"Alpha Vantage request failed with HTTP {r.status_code}\")\n",
    "\n",
    "    payload = r.json()\n",
    "\n",
    "    # Alpha Vantage returns time series under \"Time Series (Daily)\"\n",
    "    ts = payload.get(\"Time Series (Daily)\")\n",
    "    if ts is None:\n",
    "        raise RuntimeError(f\"Unexpected Alpha Vantage response: {payload}\")\n",
    "\n",
    "    records = []\n",
    "    for day_str, vals in ts.items():\n",
    "        # vals keys look like: '1. open', '2. high', etc.\n",
    "        records.append({\n",
    "            \"date\": day_str,\n",
    "            \"open\": float(vals[\"1. open\"]),\n",
    "            \"high\": float(vals[\"2. high\"]),\n",
    "            \"low\": float(vals[\"3. low\"]),\n",
    "            \"close\": float(vals[\"4. close\"]),\n",
    "            \"adjusted_close\": float(vals[\"5. adjusted close\"]),\n",
    "            \"volume\": float(vals[\"6. volume\"]),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Make sure it's sorted ascending by date\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ------------\n",
    "# Step 1: check last date we already have for this symbol\n",
    "# ------------\n",
    "last_date = get_last_date_for_symbol(SYMBOL)\n",
    "print(f\"üìÖ Last stored date for {SYMBOL} in DB:\", last_date)\n",
    "\n",
    "# ------------\n",
    "# Step 2: fetch from Alpha Vantage\n",
    "# ------------\n",
    "raw_df = fetch_alpha_vantage_daily_adjusted(SYMBOL, ALPHA_KEY)\n",
    "print(f\"‚¨áÔ∏è Pulled {len(raw_df)} rows from Alpha Vantage for {SYMBOL} \"\n",
    "      f\"({raw_df['date'].min().date()} ‚Üí {raw_df['date'].max().date()})\")\n",
    "\n",
    "# ------------\n",
    "# Step 3: filter for incremental load\n",
    "# ------------\n",
    "if last_date is not None:\n",
    "    # keep only rows with date > last_date\n",
    "    last_dt = pd.to_datetime(last_date)\n",
    "    new_df = raw_df[raw_df[\"date\"] > last_dt].copy()\n",
    "else:\n",
    "    # first time: filter by DATA_START_YEAR so we don't store ancient data if any\n",
    "    cutoff = pd.Timestamp(f\"{DATA_START_YEAR}-01-01\")\n",
    "    new_df = raw_df[raw_df[\"date\"] >= cutoff].copy()\n",
    "\n",
    "print(f\"üßÆ New rows to insert for {SYMBOL}: {len(new_df)}\")\n",
    "\n",
    "if new_df.empty:\n",
    "    print(\"‚ÑπÔ∏è No new data to insert. Skipping upsert.\")\n",
    "else:\n",
    "    # ------------\n",
    "    # Step 4: shape to match `prices` table schema\n",
    "    # ------------\n",
    "    new_df[\"symbol\"] = SYMBOL\n",
    "    new_df[\"source\"] = \"alpha_vantage\"\n",
    "\n",
    "    # ensure columns exist even if we don't yet use some technical features here\n",
    "    final_df = new_df[[\n",
    "        \"date\", \"symbol\", \"open\", \"high\", \"low\", \"close\",\n",
    "        \"volume\", \"adjusted_close\", \"source\"\n",
    "    ]].copy()\n",
    "\n",
    "    # ------------\n",
    "    # Step 5: write into SQLite\n",
    "    # ------------\n",
    "    upsert_prices(final_df)\n",
    "\n",
    "    # ------------\n",
    "    # Step 6: log ingestion\n",
    "    # ------------\n",
    "    notes = (\n",
    "        f\"symbol={SYMBOL} \"\n",
    "        f\"range={final_df['date'].min().date()}‚Üí{final_df['date'].max().date()} \"\n",
    "        f\"rows={len(final_df)}\"\n",
    "    )\n",
    "    log_ingestion(\n",
    "        source=\"alpha_vantage\",\n",
    "        action=\"fetch_prices\",\n",
    "        notes=notes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkWPSJacNS36",
    "outputId": "8f3d27e0-123d-44fd-afbf-53176e1972ee"
   },
   "outputs": [],
   "source": [
    "# Go to repo root\n",
    "%cd /content/market-forecast-capstone\n",
    "\n",
    "# Make folders if missing\n",
    "!mkdir -p notebooks data/reference\n",
    "\n",
    "# Copy your current notebook and the Excel you uploaded into the repo\n",
    "# (If your notebook is named differently, adjust the src path accordingly.)\n",
    "!cp /mnt/data/ingest_explore.ipynb notebooks/01_ingest_prices.ipynb\n",
    "!cp -n /mnt/data/StocksInSP500.xlsx data/reference/ || true\n",
    "\n",
    "# If you already created a normalized CSV during the universe step, copy it in too (ignore if not found)\n",
    "!cp -n data/reference/sp500_universe.csv data/reference/sp500_universe.csv 2>/dev/null || true\n",
    "\n",
    "# Sanity check\n",
    "!ls -lah notebooks\n",
    "!ls -lah data/reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-yjD7yGE2mV"
   },
   "outputs": [],
   "source": [
    "!git config --global user.name \"JoeGiwa\"\n",
    "!git config --global user.email \"joebot17@gmail.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiKlrNrUFMBV",
    "outputId": "966aa7de-9fc9-4d01-a71f-b54a84ba17b5"
   },
   "outputs": [],
   "source": [
    "!# FILL THESE IN:JoeGiwa/market-forecast-capstone.git\n",
    "GITHUB_USER = \"JoeGiwa\"\n",
    "\n",
    "REPO_NAME   = \"market-forecast-capstone\"\n",
    "\n",
    "TOKEN       = \"<REDACTED>\"  # keep private\n",
    "\n",
    "remote = f\"https://{TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "\n",
    "import subprocess, shlex\n",
    "\n",
    "subprocess.run(shlex.split(f\"git remote set-url origin {remote}\"), check=False)\n",
    "\n",
    "print(\"‚úÖ Remote updated (with token). Ready to push.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eCKlHcZMI41c",
    "outputId": "cbca5a8f-bc69-427b-bf7e-7ff202cf7490"
   },
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeFAIyC_J0Mw",
    "outputId": "bd57b713-b180-4352-a14c-fee91e6a6eff"
   },
   "outputs": [],
   "source": [
    "ls -lah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRcZn2ayJ9DS",
    "outputId": "f2efc23e-1cd0-4d76-fda8-69288e416cc9"
   },
   "outputs": [],
   "source": [
    "%cd /content/market-forecast-capstone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKrKaYKIKAMN",
    "outputId": "65af88f3-3182-46cb-9876-dc3eb1869614"
   },
   "outputs": [],
   "source": [
    "ls -lah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Av3zsqFKIGc",
    "outputId": "639f923b-cf69-4472-8aae-9e33d81ebfe9"
   },
   "outputs": [],
   "source": [
    "\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3f--LB-I7c9",
    "outputId": "ef66b9a7-5abe-4c44-c058-65fb3a9ce459"
   },
   "outputs": [],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMUumnQvIASL",
    "outputId": "d757dd2e-be10-418c-8957-03149a51df23"
   },
   "outputs": [],
   "source": [
    "!git add notebooks/01_ingest_prices.ipynb data/reference/StocksInSP500.xlsx\n",
    "!git commit -m \"Add ingestion notebook and S&P500 Excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHlyCJmRID9_",
    "outputId": "34ed71c1-416e-40d9-9de1-e7481ce7bab8"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
